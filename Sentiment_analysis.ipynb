{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14c4061",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfbae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import contractions\n",
    "from unidecode import unidecode\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from autocorrect import Speller\n",
    "import pandas as pd \n",
    "from gensim.models import Word2Vec\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89f7cccd",
   "metadata": {},
   "source": [
    "problem statement : classify text pos- neg (sentiment analysis)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7627c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Train.csv\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43cef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud  : \"\"\n",
    "def generate_wordcloud(data,column):\n",
    "    df = data[column].str.cat(sep=\" \") # record1   record 2 \n",
    "    text = \" \".join([word for word in df.split()])# \n",
    "    word_cloud = WordCloud(height=500,width=700,background_color='white',min_font_size=10).generate(text)\n",
    "    plt.figure(figsize=(10,16))\n",
    "    plt.imshow(word_cloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(data,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. remove blanklines,whitespaces,tabs\n",
    "# 2. contraction mapping \n",
    "# 3. Handling accented characters\n",
    "# 4. Cleaning : tokenization,remove punctuation,normalization,stopwords removal,remove numerical format,len(word)<2 removed\n",
    "# 5. autocorrect \n",
    "# 6. stemming and lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ac944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. remove blanklines,whitespaces,tabs \n",
    "def remove_spaces(data):\n",
    "    \"Function will remove all spaces\"\n",
    "    formatted_text = data.replace('\\\\n',' ').replace('\\t',' ').replace(\"\\\\\",' ')\n",
    "    return formatted_text \n",
    "# 2. Contraction mapping \n",
    "def expand_text(data):\n",
    "    fixed_text = contractions.fix(data)\n",
    "    return fixed_text\n",
    "\n",
    "# 3. Handling accented \n",
    "def handling_accented(data):\n",
    "    fixed_text = unidecode(data)\n",
    "    return fixed_text\n",
    "# 4.cleaning \n",
    "stopword_list = stopwords.words(\"english\")\n",
    "stopword_list.remove(\"not\")\n",
    "stopword_list.remove(\"nor\")\n",
    "stopword_list.remove(\"no\")\n",
    "def cleaning(data):\n",
    "    tokens = word_tokenize(data) # tokenization\n",
    "    clean_text = [word.lower() for word in tokens if (word not in stopword_list) and(word not in punctuation) and(len(word)>2) and(word.isalpha())]\n",
    "    return clean_text\n",
    "\n",
    "# 5. autocorrect \n",
    "def autocorrection(data):\n",
    "    spell = Speller(lang='en')\n",
    "    corrected_text = spell(data)\n",
    "    return corrected_text\n",
    "\n",
    "# 6. Lemmatization \n",
    "def lemmatization(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    final_text = []\n",
    "    for word in data:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        final_text.append(lemmatized_word)\n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data leakage \n",
    "x_train,x_test,y_train,y_test = train_test_split(data.text,data.label,test_size=0.25,random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2aaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "clean_text_train = x_train.apply(remove_spaces)\n",
    "clean_text_test = x_test.apply(remove_spaces)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(expand_text)\n",
    "clean_text_test = clean_text_test.apply(expand_text)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(handling_accented)\n",
    "clean_text_test = clean_text_test.apply(handling_accented)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(cleaning)\n",
    "clean_text_test = clean_text_test.apply(cleaning)\n",
    "\n",
    "clean_text_train = clean_text_train.apply(lemmatization)\n",
    "clean_text_test = clean_text_test.apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df12886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_2(data):\n",
    "    tokens = data.split()\n",
    "    clean_text = [word for word in tokens if word not in stopword_list]\n",
    "    return \" \".join(clean_text)\n",
    "clean_text_train = clean_text_train.apply(filter_2)\n",
    "clean_text_test = clean_text_test.apply(filter_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c148bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implememtation of ngrams \n",
    "from nltk.util import ngrams\n",
    "for unigrams in ngrams(clean_text_train[11449].split(),2):\n",
    "    print(\" \".join(unigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ca2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer \n",
    "count = CountVectorizer(max_df = 0.95,max_features=1000)\n",
    "count_val_train =count.fit_transform(clean_text_train)\n",
    "count_val_test = count.transform(clean_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22cf147",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d721eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(count_val_train.A,columns=count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1500ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mnb = MultinomialNB()\n",
    "count_mnb.fit(count_val_train.A,y_train) \n",
    "predict_mnb = count_mnb.predict(count_val_test.A)\n",
    "accuracy_count = accuracy_score(y_test,predict_mnb)*100\n",
    "accuracy_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2682423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF \n",
    "tfidf = TfidfVectorizer(max_df = 0.95,max_features=1000)\n",
    "tfidf_train = tfidf.fit_transform(clean_text_train)\n",
    "tfidf_test = tfidf.transform(clean_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc6c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tfidf_train.A,columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87470e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mnb = MultinomialNB()\n",
    "tfidf_mnb.fit(tfidf_train.A,y_train)\n",
    "predict_tfidf = tfidf_mnb.predict(tfidf_test.A)\n",
    "accuracy_tfidf = accuracy_score(y_test,predict_tfidf)*100\n",
    "print(f\" accuracy_tfidf {accuracy_tfidf}\")\n",
    "print(f\" accuracy_count {accuracy_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71187604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
